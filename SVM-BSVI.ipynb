{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM25IpK5W6rU9UQqChpF0j5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ReginaldAboagye/365datascience/blob/master/SVM-BSVI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: SVM with Adaptive Kernel\n",
        "First, let's define an SVM with a Gaussian kernel where the width parameter\n",
        "ðœŽ\n",
        "Ïƒ is adapted based on volatility estimates."
      ],
      "metadata": {
        "id": "HYuhpEMDKtA6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zA2Qu5lYKsTn",
        "outputId": "8ec40222-7119-474b-b47b-188b56b8078a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1.  1.  1. -1. -1.]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def gaussian_kernel(x, y, sigma):\n",
        "    return np.exp(-np.linalg.norm(x-y)**2 / (2 * sigma**2))\n",
        "\n",
        "class SVM:\n",
        "    def __init__(self, C=1.0):\n",
        "        self.C = C\n",
        "        self.alpha = None\n",
        "        self.b = 0\n",
        "        self.support_vectors = None\n",
        "        self.y_sv = None\n",
        "\n",
        "    def fit(self, X, y, sigma_func):\n",
        "        n_samples, n_features = X.shape\n",
        "        K = np.zeros((n_samples, n_samples))\n",
        "        for i in range(n_samples):\n",
        "            for j in range(n_samples):\n",
        "                K[i, j] = gaussian_kernel(X[i], X[j], sigma_func(i, j))\n",
        "\n",
        "        P = np.outer(y, y) * K\n",
        "        q = -np.ones(n_samples)\n",
        "        G = np.vstack((-np.eye(n_samples), np.eye(n_samples)))\n",
        "        h = np.hstack((np.zeros(n_samples), np.ones(n_samples) * self.C))\n",
        "        A = y.reshape(1, -1)\n",
        "        b = np.zeros(1)\n",
        "\n",
        "        alpha = self.solve_qp(P, q, G, h, A, b)\n",
        "        sv = alpha > 1e-5\n",
        "        self.alpha = alpha[sv]\n",
        "        self.support_vectors = X[sv]\n",
        "        self.y_sv = y[sv]\n",
        "        self.b = np.mean(self.y_sv - np.sum(self.alpha * self.y_sv * K[sv][:, sv], axis=1))\n",
        "\n",
        "    def predict(self, X, sigma_func):\n",
        "        y_pred = []\n",
        "        for x in X:\n",
        "            prediction = sum(self.alpha[i] * self.y_sv[i] * gaussian_kernel(x, self.support_vectors[i], sigma_func(i, i))\n",
        "                             for i in range(len(self.alpha))) + self.b\n",
        "            y_pred.append(np.sign(prediction))\n",
        "        return np.array(y_pred)\n",
        "\n",
        "    def solve_qp(self, P, q, G, h, A, b):\n",
        "        # Placeholder for QP solver implementation\n",
        "        return np.random.rand(P.shape[0])  # Dummy implementation\n",
        "\n",
        "# Example usage\n",
        "X = np.array([[1, 2], [2, 3], [3, 3], [2, 1], [3, 2]])\n",
        "y = np.array([1, 1, 1, -1, -1])\n",
        "sigma_func = lambda i, j: 1.0  # Dummy sigma function\n",
        "\n",
        "svm = SVM()\n",
        "svm.fit(X, y, sigma_func)\n",
        "predictions = svm.predict(X, sigma_func)\n",
        "print(predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hamiltonian Monte Carlo (HMC)**\n",
        "HMC leverages the gradient of the log-posterior to propose new states in a more informed manner compared to traditional MCMC methods. The key steps in HMC include:\n",
        "\n",
        "*Initialization:* Set initial values for the parameters.\n",
        "\n",
        "*Leapfrog Integrator:* Use the leapfrog method to simulate Hamiltonian dynamics.\n",
        "\n",
        "*Metropolis-Hastings Step:* Accept or reject the proposed move based on the Hamiltonian."
      ],
      "metadata": {
        "id": "A0j3qjZ_LAUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class EKF:\n",
        "    def __init__(self, F, H, Q, R):\n",
        "        self.F = F\n",
        "        self.H = H\n",
        "        self.Q = Q\n",
        "        self.R = R\n",
        "\n",
        "    def predict(self, x, P):\n",
        "        x_pred = self.F @ x\n",
        "        P_pred = self.F @ P @ self.F.T + self.Q\n",
        "        return x_pred, P_pred\n",
        "\n",
        "    def update(self, x_pred, P_pred, y):\n",
        "        y_pred = self.H @ x_pred\n",
        "        S = self.H @ P_pred @ self.H.T + self.R\n",
        "        K = P_pred @ self.H.T @ np.linalg.inv(S)\n",
        "        x_upd = x_pred + K @ (y - y_pred)\n",
        "        P_upd = P_pred - K @ self.H @ P_pred\n",
        "        return x_upd, P_upd\n",
        "\n",
        "    def run(self, y, x0, P0):\n",
        "        n = len(y)\n",
        "        x_est = np.zeros((n, len(x0)))\n",
        "        x_est[0] = x0\n",
        "        P = P0\n",
        "        for t in range(1, n):\n",
        "            x_pred, P_pred = self.predict(x_est[t-1], P)\n",
        "            x_est[t], P = self.update(x_pred, P_pred, y[t])\n",
        "        return x_est\n",
        "\n",
        "class BayesianStochasticVolatilityEKF_HMC:\n",
        "    def __init__(self, F, H, Q, R, mu_prior, phi_prior, sigma_eta_prior, n_iter=1000, epsilon=0.01, L=10):\n",
        "        self.ekf = EKF(F, H, Q, R)\n",
        "        self.mu_prior = mu_prior\n",
        "        self.phi_prior = phi_prior\n",
        "        self.sigma_eta_prior = sigma_eta_prior\n",
        "        self.n_iter = n_iter\n",
        "        self.epsilon = epsilon  # Step size\n",
        "        self.L = L  # Number of leapfrog steps\n",
        "\n",
        "    def potential_energy(self, params, y):\n",
        "        mu, phi, sigma_eta = params\n",
        "        sigma_eta = sigma_eta if np.isscalar(sigma_eta) else sigma_eta.item()  # Ensure sigma_eta is a scalar\n",
        "        if sigma_eta <= 0:\n",
        "            return np.inf\n",
        "        F = np.array([[phi]])\n",
        "        H = np.array([[1]])\n",
        "        Q = np.array([[sigma_eta**2]])\n",
        "        R = np.array([[1]])  # Assume observation noise is 1 for simplicity\n",
        "\n",
        "        x0 = np.array([mu])\n",
        "        P0 = np.array([[sigma_eta**2]])\n",
        "\n",
        "        x_est = self.ekf.run(y, x0, P0)\n",
        "\n",
        "        likelihood = -0.5 * np.sum((y - x_est.flatten()) ** 2)\n",
        "        prior = -0.5 * (mu - self.mu_prior[0]) ** 2 / self.mu_prior[1] ** 2 \\\n",
        "                -0.5 * (phi - self.phi_prior[0]) ** 2 / self.phi_prior[1] ** 2 \\\n",
        "                -0.5 * (sigma_eta - self.sigma_eta_prior[0]) ** 2 / self.sigma_eta_prior[1] ** 2\n",
        "        return -(likelihood + prior)\n",
        "\n",
        "    def gradient_potential_energy(self, params, y):\n",
        "        mu, phi, sigma_eta = params\n",
        "        sigma_eta = np.atleast_1d(sigma_eta)[0]  # Convert sigma_eta to a scalar\n",
        "\n",
        "        F = np.array([[phi]])\n",
        "        H = np.array([[1]])\n",
        "        Q = np.array([[sigma_eta**2]])\n",
        "        R = np.array([[1]])\n",
        "\n",
        "        x0 = np.array([mu])\n",
        "        P0 = np.array([[sigma_eta**2]])\n",
        "\n",
        "        x_est = self.ekf.run(y, x0, P0)\n",
        "\n",
        "        grad_mu = 0\n",
        "        grad_phi = 0\n",
        "        grad_sigma_eta = 0\n",
        "        for t in range(1, len(y)):\n",
        "            grad_mu += (y[t] - x_est[t]) / np.exp(x_est[t])\n",
        "            grad_phi += (y[t] - x_est[t]) * (x_est[t-1] - mu) / np.exp(x_est[t])\n",
        "            grad_sigma_eta += (y[t] - mu - phi * y[t-1])**2 / (2 * sigma_eta**3)\n",
        "\n",
        "        grad_mu -= (mu - self.mu_prior[0]) / self.mu_prior[1] ** 2\n",
        "        grad_phi -= (phi - self.phi_prior[0]) / self.phi_prior[1] ** 2\n",
        "        grad_sigma_eta -= (sigma_eta - self.sigma_eta_prior[0]) / self.sigma_eta_prior[1] ** 2\n",
        "\n",
        "        return np.array([grad_mu, grad_phi, grad_sigma_eta])\n",
        "\n",
        "    def leapfrog(self, params, momentum, grad, epsilon):\n",
        "        momentum_half_step = momentum - 0.5 * epsilon * grad\n",
        "        params_full_step = params + epsilon * momentum_half_step\n",
        "        grad_new = self.gradient_potential_energy(params_full_step, log_returns)\n",
        "        momentum_full_step = momentum_half_step - 0.5 * epsilon * grad_new\n",
        "        return params_full_step, momentum_full_step, grad_new\n",
        "\n",
        "    def hmc_step(self, params, y):\n",
        "        momentum = np.random.normal(size=params.shape)\n",
        "        current_params = params.copy()\n",
        "        current_momentum = momentum.copy()\n",
        "        current_grad = self.gradient_potential_energy(params, y)\n",
        "        proposed_params = params.copy()\n",
        "        proposed_momentum = momentum.copy()\n",
        "\n",
        "        for _ in range(self.L):\n",
        "            proposed_params, proposed_momentum, current_grad = self.leapfrog(proposed_params, proposed_momentum, current_grad, self.epsilon)\n",
        "\n",
        "        current_potential = self.potential_energy(current_params, y)\n",
        "        current_kinetic = 0.5 * np.sum(current_momentum ** 2)\n",
        "        proposed_potential = self.potential_energy(proposed_params, y)\n",
        "        proposed_kinetic = 0.5 * np.sum(proposed_momentum ** 2)\n",
        "\n",
        "        if np.random.uniform() < np.exp(current_potential - proposed_potential + current_kinetic - proposed_kinetic):\n",
        "            return proposed_params\n",
        "        else:\n",
        "            return current_params\n",
        "\n",
        "    def fit(self, y):\n",
        "        params = np.array([self.mu_prior[0], self.phi_prior[0], self.sigma_eta_prior[0]])\n",
        "        samples = np.zeros((self.n_iter, 3))\n",
        "        for i in range(self.n_iter):\n",
        "            params = self.hmc_step(params, y)\n",
        "            samples[i] = params\n",
        "        return samples\n",
        "\n",
        "# Example usage\n",
        "log_returns = np.random.normal(0, 0.1, 100)\n",
        "F = np.array([[0.9]])  # Initial transition matrix\n",
        "H = np.array([[1]])    # Observation matrix\n",
        "Q = np.array([[0.01]]) # Process noise covariance\n",
        "R = np.array([[1]])    # Observation noise covariance\n",
        "\n",
        "hmc_ekf_model = BayesianStochasticVolatilityEKF_HMC(F, H, Q, R, mu_prior=(0, 1), phi_prior=(0.9, 0.1), sigma_eta_prior=(0.1, 0.01), n_iter=1000, epsilon=0.01, L=10)\n",
        "samples = hmc_ekf_model.fit(log_returns)\n",
        "\n",
        "print(\"Posterior mean estimates\")\n",
        "print(\"mu:\", np.mean(samples[:, 0]))\n",
        "print(\"phi:\", np.mean(samples[:, 1]))\n",
        "print(\"sigma_eta:\", np.mean(samples[:, 2]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "4LgZzes8iAHo",
        "outputId": "2925b1ce-ced0-4e51-c58d-7fd2b046fe18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-d56a7381fb58>\u001b[0m in \u001b[0;36m<cell line: 136>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0mhmc_ekf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBayesianStochasticVolatilityEKF_HMC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_eta_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhmc_ekf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_returns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Posterior mean estimates\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-d56a7381fb58>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhmc_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-d56a7381fb58>\u001b[0m in \u001b[0;36mhmc_step\u001b[0;34m(self, params, y)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mcurrent_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mcurrent_momentum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mcurrent_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_potential_energy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0mproposed_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mproposed_momentum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-d56a7381fb58>\u001b[0m in \u001b[0;36mgradient_potential_energy\u001b[0;34m(self, params, y)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mgrad_sigma_eta\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msigma_eta\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma_eta_prior\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma_eta_prior\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrad_mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_phi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_sigma_eta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mleapfrog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class EKF:\n",
        "    def __init__(self, F, H, Q, R):\n",
        "        self.F = F\n",
        "        self.H = H\n",
        "        self.Q = Q\n",
        "        self.R = R\n",
        "\n",
        "    def predict(self, x, P):\n",
        "        x_pred = self.F @ x\n",
        "        P_pred = self.F @ P @ self.F.T + self.Q\n",
        "        return x_pred, P_pred\n",
        "\n",
        "    def update(self, x_pred, P_pred, y):\n",
        "        y_pred = self.H @ x_pred\n",
        "        S = self.H @ P_pred @ self.H.T + self.R\n",
        "        K = P_pred @ self.H.T @ np.linalg.inv(S)\n",
        "        x_upd = x_pred + K @ (y - y_pred)\n",
        "        P_upd = P_pred - K @ self.H @ P_pred\n",
        "        return x_upd, P_upd\n",
        "\n",
        "    def run(self, y, x0, P0):\n",
        "        n = len(y)\n",
        "        x_est = np.zeros((n, len(x0)))\n",
        "        x_est[0] = x0\n",
        "        P = P0\n",
        "        for t in range(1, n):\n",
        "            x_pred, P_pred = self.predict(x_est[t-1], P)\n",
        "            x_est[t], P = self.update(x_pred, P_pred, y[t])\n",
        "        return x_est\n",
        "\n",
        "class BayesianStochasticVolatilityEKF_HMC:\n",
        "    def __init__(self, F, H, Q, R, mu_prior, phi_prior, sigma_eta_prior, n_iter=1000, epsilon=0.01, L=10):\n",
        "        self.ekf = EKF(F, H, Q, R)\n",
        "        self.mu_prior = mu_prior\n",
        "        self.phi_prior = phi_prior\n",
        "        self.sigma_eta_prior = sigma_eta_prior\n",
        "        self.n_iter = n_iter\n",
        "        self.epsilon = epsilon  # Step size\n",
        "        self.L = L  # Number of leapfrog steps\n",
        "\n",
        "    def potential_energy(self, params, y):\n",
        "        mu, phi, sigma_eta = params\n",
        "        sigma_eta = np.asscalar(sigma_eta) if isinstance(sigma_eta, np.ndarray) else sigma_eta  # Ensure sigma_eta is a scalar\n",
        "        if sigma_eta <= 0:\n",
        "            return np.inf\n",
        "        F = np.array([[phi]])\n",
        "        H = np.array([[1]])\n",
        "        Q = np.array([[sigma_eta**2]])\n",
        "        R = np.array([[1]])  # Assume observation noise is 1 for simplicity\n",
        "\n",
        "        x0 = np.array([mu])\n",
        "        P0 = np.array([[sigma_eta**2]])\n",
        "\n",
        "        x_est = self.ekf.run(y, x0, P0)\n",
        "\n",
        "        likelihood = -0.5 * np.sum((y - x_est.flatten()) ** 2)\n",
        "        prior = -0.5 * (mu - self.mu_prior[0]) ** 2 / self.mu_prior[1] ** 2 \\\n",
        "                -0.5 * (phi - self.phi_prior[0]) ** 2 / self.phi_prior[1] ** 2 \\\n",
        "                -0.5 * (sigma_eta - self.sigma_eta_prior[0]) ** 2 / self.sigma_eta_prior[1] ** 2\n",
        "        return -(likelihood + prior)\n",
        "\n",
        "    def gradient_potential_energy(self, params, y):\n",
        "        mu, phi, sigma_eta = params\n",
        "        sigma_eta = np.asscalar(sigma_eta) if isinstance(sigma_eta, np.ndarray) else sigma_eta  # Ensure sigma_eta is a scalar\n",
        "        if sigma_eta <= 0:\n",
        "            return np.array([0, 0, 0])\n",
        "        F = np.array([[phi]])\n",
        "        H = np.array([[1]])\n",
        "        Q = np.array([[sigma_eta**2]])\n",
        "        R = np.array([[1]])\n",
        "\n",
        "        x0 = np.array([mu])\n",
        "        P0 = np.array([[sigma_eta**2]])\n",
        "\n",
        "        x_est = self.ekf.run(y, x0, P0)\n",
        "\n",
        "        grad_mu = grad_phi = grad_sigma_eta = 0\n",
        "        for t in range(1, len(y)):\n",
        "            grad_mu += (y[t] - x_est[t]) / np.exp(x_est[t])\n",
        "            grad_phi += (y[t] - x_est[t]) * (x_est[t-1] - mu) / np.exp(x_est[t])\n",
        "            grad_sigma_eta += (y[t] - x_est[t]) * np.random.normal(0, sigma_eta) / np.exp(x_est[t])\n",
        "\n",
        "        grad_mu -= (mu - self.mu_prior[0]) / self.mu_prior[1] ** 2\n",
        "        grad_phi -= (phi - self.phi_prior[0]) / self.phi_prior[1] ** 2\n",
        "        grad_sigma_eta -= (sigma_eta - self.sigma_eta_prior[0]) / self.sigma_eta_prior[1] ** 2\n",
        "\n",
        "        return np.array([-grad_mu, -grad_phi, -grad_sigma_eta])\n",
        "\n",
        "    def leapfrog(self, params, momentum, grad, epsilon):\n",
        "        momentum_half_step = momentum - 0.5 * epsilon * grad\n",
        "        params_full_step = params + epsilon * momentum_half_step\n",
        "        grad_new = self.gradient_potential_energy(params_full_step, log_returns)\n",
        "        momentum_full_step = momentum_half_step - 0.5 * epsilon * grad_new\n",
        "\n",
        "        # Ensure parameters stay within valid ranges\n",
        "        params_full_step[2] = max(params_full_step[2], 1e-6)  # sigma_eta must be positive\n",
        "\n",
        "        return params_full_step, momentum_full_step, grad_new\n",
        "\n",
        "    def hmc_step(self, params, y):\n",
        "        momentum = np.random.normal(size=params.shape)\n",
        "        current_params = params.copy()\n",
        "        current_momentum = momentum.copy()\n",
        "        current_grad = self.gradient_potential\n",
        "        current_grad = self.gradient_potential_energy(params, y)\n",
        "        proposed_params = params.copy()\n",
        "        proposed_momentum = momentum.copy()\n",
        "\n",
        "        for _ in range(self.L):\n",
        "            proposed_params, proposed_momentum, current_grad = self.leapfrog(proposed_params, proposed_momentum, current_grad, self.epsilon)\n",
        "\n",
        "        current_potential = self.potential_energy(current_params, y)\n",
        "        current_kinetic = 0.5 * np.sum(current_momentum ** 2)\n",
        "        proposed_potential = self.potential_energy(proposed_params, y)\n",
        "        proposed_kinetic = 0.5 * np.sum(proposed_momentum ** 2)\n",
        "\n",
        "        if np.random.uniform() < np.exp(current_potential - proposed_potential + current_kinetic - proposed_kinetic):\n",
        "            return proposed_params\n",
        "        else:\n",
        "            return current_params\n",
        "\n",
        "    def fit(self, y):\n",
        "        params = np.array([self.mu_prior[0], self.phi_prior[0], self.sigma_eta_prior[0]])\n",
        "        samples = np.zeros((self.n_iter, 3))\n",
        "        for i in range(self.n_iter):\n",
        "            params = self.hmc_step(params, y)\n",
        "            samples[i] = params\n",
        "        return samples\n",
        "\n",
        "# Example usage\n",
        "log_returns = np.random.normal(0, 0.1, 100)\n",
        "F = np.array([[0.9]])  # Initial transition matrix\n",
        "H = np.array([[1]])    # Observation matrix\n",
        "Q = np.array([[0.01]]) # Process noise covariance\n",
        "R = np.array([[1]])    # Observation noise covariance\n",
        "\n",
        "hmc_ekf_model = BayesianStochasticVolatilityEKF_HMC(F, H, Q, R, mu_prior=(0, 1), phi_prior=(0.9, 0.1), sigma_eta_prior=(0.1, 0.01), n_iter=1000, epsilon=0.01, L=10)\n",
        "samples = hmc_ekf_model.fit(log_returns)\n",
        "\n",
        "print(\"Posterior mean estimates\")\n",
        "print(\"mu:\", np.mean(samples[:, 0]))\n",
        "print(\"phi:\", np.mean(samples[:, 1]))\n",
        "print(\"sigma_eta:\", np.mean(samples[:, 2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "FIdH1Kt8UwXp",
        "outputId": "a0441bfc-97e8-4df3-d2ec-45bb095b7e55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'BayesianStochasticVolatilityEKF_HMC' object has no attribute 'gradient_potential'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-533a9a93a0e3>\u001b[0m in \u001b[0;36m<cell line: 140>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0mhmc_ekf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBayesianStochasticVolatilityEKF_HMC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_eta_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhmc_ekf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_returns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Posterior mean estimates\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-533a9a93a0e3>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhmc_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m             \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-533a9a93a0e3>\u001b[0m in \u001b[0;36mhmc_step\u001b[0;34m(self, params, y)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mcurrent_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mcurrent_momentum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mcurrent_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_potential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mcurrent_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_potential_energy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mproposed_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'BayesianStochasticVolatilityEKF_HMC' object has no attribute 'gradient_potential'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class EKF:\n",
        "    def __init__(self, F, H, Q, R):\n",
        "        self.F = F\n",
        "        self.H = H\n",
        "        self.Q = Q\n",
        "        self.R = R\n",
        "\n",
        "    def predict(self, x, P):\n",
        "        x_pred = self.F @ x\n",
        "        P_pred = self.F @ P @ self.F.T + self.Q\n",
        "        return x_pred, P_pred\n",
        "\n",
        "    def update(self, x_pred, P_pred, y):\n",
        "        y_pred = self.H @ x_pred\n",
        "        S = self.H @ P_pred @ self.H.T + self.R\n",
        "        K = P_pred @ self.H.T @ np.linalg.inv(S)\n",
        "        x_upd = x_pred + K @ (y - y_pred)\n",
        "        P_upd = P_pred - K @ self.H @ P_pred\n",
        "        return x_upd, P_upd\n",
        "\n",
        "    def run(self, y, x0, P0):\n",
        "        n = len(y)\n",
        "        x_est = np.zeros((n, len(x0)))\n",
        "        x_est[0] = x0\n",
        "        P = P0\n",
        "        for t in range(1, n):\n",
        "            x_pred, P_pred = self.predict(x_est[t-1], P)\n",
        "            x_est[t], P = self.update(x_pred, P_pred, y[t])\n",
        "        return x_est\n",
        "\n",
        "class BayesianStochasticVolatilityEKF_HMC:\n",
        "    def __init__(self, F, H, Q, R, mu_prior, phi_prior, sigma_eta_prior, n_iter=1000, epsilon=0.01, L=10):\n",
        "        self.ekf = EKF(F, H, Q, R)\n",
        "        self.mu_prior = mu_prior\n",
        "        self.phi_prior = phi_prior\n",
        "        self.sigma_eta_prior = sigma_eta_prior\n",
        "        self.n_iter = n_iter\n",
        "        self.epsilon = epsilon  # Step size\n",
        "        self.L = L  # Number of leapfrog steps\n",
        "\n",
        "    def potential_energy(self, params, y):\n",
        "        mu, phi, sigma_eta = params\n",
        "        sigma_eta = sigma_eta if np.isscalar(sigma_eta) else sigma_eta.item()  # Ensure sigma_eta is a scalar\n",
        "        if sigma_eta <= 0:\n",
        "            return np.inf\n",
        "        F = np.array([[phi]])\n",
        "        H = np.array([[1]])\n",
        "        Q = np.array([[sigma_eta**2]])\n",
        "        R = np.array([[1]])  # Assume observation noise is 1 for simplicity\n",
        "\n",
        "        x0 = np.array([mu])\n",
        "        P0 = np.array([[sigma_eta**2]])\n",
        "\n",
        "        x_est = self.ekf.run(y, x0, P0)\n",
        "\n",
        "        likelihood = -0.5 * np.sum((y - x_est.flatten()) ** 2)\n",
        "        prior = -0.5 * (mu - self.mu_prior[0]) ** 2 / self.mu_prior[1] ** 2 \\\n",
        "                -0.5 * (phi - self.phi_prior[0]) ** 2 / self.phi_prior[1] ** 2 \\\n",
        "                -0.5 * (sigma_eta - self.sigma_eta_prior[0]) ** 2 / self.sigma_eta_prior[1] ** 2\n",
        "        return -(likelihood + prior)\n",
        "\n",
        "    def gradient_potential_energy(self, params, y):\n",
        "        mu, phi, sigma_eta = params\n",
        "        sigma_eta = np.atleast_1d(sigma_eta)[0]  # Convert sigma_eta to a scalar\n",
        "\n",
        "        F = np.array([[phi]])\n",
        "        H = np.array([[1]])\n",
        "        Q = np.array([[sigma_eta**2]])\n",
        "        R = np.array([[1]])\n",
        "\n",
        "        n = len(y)\n",
        "        gradient_mu = np.zeros(1)\n",
        "        gradient_phi = np.zeros(1)\n",
        "        gradient_sigma_eta = np.zeros(1)\n",
        "\n",
        "\n",
        "        x0 = np.array([mu])\n",
        "        P0 = np.array([[sigma_eta**2]])\n",
        "\n",
        "        x_est = self.ekf.run(y, x0, P0)\n",
        "\n",
        "\n",
        "        grad_mu = grad_phi = grad_sigma_eta = 0\n",
        "        for t in range(1, len(y)):\n",
        "            grad_mu += (y[t] - x_est[t]) / np.exp(x_est[t])\n",
        "            grad_phi += (y[t] - x_est[t]) * (x_est[t-1] - mu) / np.exp(x_est[t])\n",
        "            gradient_sigma_eta += (y[t] - mu - phi * y[t-1])**2 / (2 * sigma_eta**3)\n",
        "\n",
        "        grad_mu -= (mu - self.mu_prior[0]) / self.mu_prior[1] ** 2\n",
        "        grad_phi -= (phi - self.phi_prior[0]) / self.phi_prior[1] ** 2\n",
        "        grad_sigma_eta -= (sigma_eta - self.sigma_eta_prior[0]) / self.sigma_eta_prior[1] ** 2\n",
        "\n",
        "        return np.array([-grad_mu, -grad_phi, -grad_sigma_eta])\n",
        "\n",
        "    def leapfrog(self, params, momentum, grad, epsilon):\n",
        "        momentum_half_step = momentum - 0.5 * epsilon * grad\n",
        "        params_full_step = params + epsilon * momentum_half_step\n",
        "        grad_new = self.gradient_potential_energy(np.squeeze(params_full_step), log_returns)\n",
        "        momentum_full_step = momentum_half_step - 0.5 * epsilon * grad_new\n",
        "        return params_full_step, momentum_full_step, grad_new\n",
        "\n",
        "    def hmc_step(self, params, y):\n",
        "        momentum = np.random.normal(size=params.shape)\n",
        "        current_params = params.copy()\n",
        "        current_momentum = momentum.copy()\n",
        "        current_grad = self.gradient_potential_energy(params, y)\n",
        "        proposed_params = params.copy()\n",
        "        proposed_momentum = momentum.copy()\n",
        "\n",
        "        for _ in range(self.L):\n",
        "            proposed_params, proposed_momentum, current_grad = self.leapfrog(proposed_params, proposed_momentum, current_grad, self.epsilon)\n",
        "\n",
        "        current_potential = self.potential_energy(current_params, y)\n",
        "        current_kinetic = 0.5 * np.sum(current_momentum ** 2)\n",
        "        proposed_potential = self.potential_energy(proposed_params, y)\n",
        "        proposed_kinetic = 0.5 * np.sum(proposed_momentum ** 2)\n",
        "\n",
        "        if np.random.uniform() < np.exp(current_potential - proposed_potential + current_kinetic - proposed_kinetic):\n",
        "            return proposed_params\n",
        "        else:\n",
        "            return current_params\n",
        "\n",
        "    def fit(self, y):\n",
        "        params = np.array([self.mu_prior[0], self.phi_prior[0], self.sigma_eta_prior[0]])\n",
        "        samples = np.zeros((self.n_iter, 3))\n",
        "        for i in range(self.n_iter):\n",
        "            params = self.hmc_step(params, y)\n",
        "            samples[i] = params\n",
        "        return samples\n",
        "\n",
        "# Example usage\n",
        "log_returns = np.random.normal(0, 0.1, 100)\n",
        "F = np.array([[0.9]])  # Initial transition matrix\n",
        "H = np.array([[1]])    # Observation matrix\n",
        "Q = np.array([[0.01]]) # Process noise covariance\n",
        "R = np.array([[1]])    # Observation noise covariance\n",
        "\n",
        "hmc_ekf_model = BayesianStochasticVolatilityEKF_HMC(F, H, Q, R, mu_prior=(0, 1), phi_prior=(0.9, 0.1), sigma_eta_prior=(0.1, 0.01), n_iter=1000, epsilon=0.01, L=10)\n",
        "samples = hmc_ekf_model.fit(log_returns)\n",
        "\n",
        "print(\"Posterior mean estimates\")\n",
        "print(\"mu:\", np.mean(samples[:, 0]))\n",
        "print(\"phi:\", np.mean(samples[:, 1]))\n",
        "print(\"sigma_eta:\", np.mean(samples[:, 2]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "BFIvw8sqbh53",
        "outputId": "b47d6c7f-028d-47ab-d005-2b48dd226331"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-cdeeacacf2dc>\u001b[0m in \u001b[0;36m<cell line: 141>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0mhmc_ekf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBayesianStochasticVolatilityEKF_HMC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_eta_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhmc_ekf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_returns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Posterior mean estimates\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-cdeeacacf2dc>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhmc_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m             \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-cdeeacacf2dc>\u001b[0m in \u001b[0;36mhmc_step\u001b[0;34m(self, params, y)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mcurrent_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mcurrent_momentum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mcurrent_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_potential_energy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mproposed_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mproposed_momentum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-cdeeacacf2dc>\u001b[0m in \u001b[0;36mgradient_potential_energy\u001b[0;34m(self, params, y)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mgrad_sigma_eta\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msigma_eta\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma_eta_prior\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma_eta_prior\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgrad_mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mgrad_phi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mgrad_sigma_eta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mleapfrog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class EKF:\n",
        "    def __init__(self, F, H, Q, R):\n",
        "        self.F = F\n",
        "        self.H = H\n",
        "        self.Q = Q\n",
        "        self.R = R\n",
        "\n",
        "    def predict(self, x, P):\n",
        "        x_pred = self.F @ x\n",
        "        P_pred = self.F @ P @ self.F.T + self.Q\n",
        "        return x_pred, P_pred\n",
        "\n",
        "    def update(self, x_pred, P_pred, y):\n",
        "        y_pred = self.H @ x_pred\n",
        "        S = self.H @ P_pred @ self.H.T + self.R\n",
        "        K = P_pred @ self.H.T @ np.linalg.inv(S)\n",
        "        x_upd = x_pred + K @ (y - y_pred)\n",
        "        P_upd = P_pred - K @ self.H @ P_pred\n",
        "        return x_upd, P_upd\n",
        "\n",
        "    def run(self, y, x0, P0):\n",
        "        n = len(y)\n",
        "        x_est = np.zeros((n, len(x0)))\n",
        "        x_est[0] = x0\n",
        "        P = P0\n",
        "        for t in range(1, n):\n",
        "            x_pred, P_pred = self.predict(x_est[t-1], P)\n",
        "            x_est[t], P = self.update(x_pred, P_pred, y[t])\n",
        "        return x_est\n",
        "\n",
        "class BayesianStochasticVolatilityEKF_HMC:\n",
        "    def __init__(self, F, H, Q, R, mu_prior, phi_prior, sigma_eta_prior, n_iter=1000, epsilon=0.01, L=10):\n",
        "        self.ekf = EKF(F, H, Q, R)\n",
        "        self.mu_prior = mu_prior\n",
        "        self.phi_prior = phi_prior\n",
        "        self.sigma_eta_prior = sigma_eta_prior\n",
        "        self.n_iter = n_iter\n",
        "        self.epsilon = epsilon  # Step size\n",
        "        self.L = L  # Number of leapfrog steps\n",
        "\n",
        "    def potential_energy(self, params, y):\n",
        "        mu, phi, sigma_eta = params\n",
        "        sigma_eta = sigma_eta if np.isscalar(sigma_eta) else sigma_eta.item()  # Ensure sigma_eta is a scalar\n",
        "        if sigma_eta <= 0:\n",
        "            return np.inf\n",
        "        F = np.array([[phi]])\n",
        "        H = np.array([[1]])\n",
        "        Q = np.array([[sigma_eta**2]])\n",
        "        R = np.array([[1]])  # Assume observation noise is 1 for simplicity\n",
        "\n",
        "        x0 = np.array([mu])\n",
        "        P0 = np.array([[sigma_eta**2]])\n",
        "\n",
        "        x_est = self.ekf.run(y, x0, P0)\n",
        "\n",
        "        likelihood = -0.5 * np.sum((y - x_est.flatten()) ** 2)\n",
        "        prior = -0.5 * (mu - self.mu_prior[0]) ** 2 / self.mu_prior[1] ** 2 \\\n",
        "                -0.5 * (phi - self.phi_prior[0]) ** 2 / self.phi_prior[1] ** 2 \\\n",
        "                -0.5 * (sigma_eta - self.sigma_eta_prior[0]) ** 2 / self.sigma_eta_prior[1] ** 2\n",
        "        return -(likelihood + prior)\n",
        "\n",
        "    def gradient_potential_energy(self, params, y):\n",
        "        mu, phi, sigma_eta = params\n",
        "        sigma_eta = np.atleast_1d(sigma_eta)[0]  # Convert sigma_eta to a scalar\n",
        "\n",
        "        F = np.array([[phi]])\n",
        "        H = np.array([[1]])\n",
        "        Q = np.array([[sigma_eta**2]])\n",
        "        R = np.array([[1]])\n",
        "\n",
        "        x0 = np.array([mu])\n",
        "        P0 = np.array([[sigma_eta**2]])\n",
        "\n",
        "        x_est = self.ekf.run(y, x0, P0)\n",
        "\n",
        "        grad_mu = 0\n",
        "        grad_phi = 0\n",
        "        grad_sigma_eta = 0\n",
        "        for t in range(1, len(y)):\n",
        "            grad_mu += (y[t] - x_est[t]) / np.exp(x_est[t])\n",
        "            grad_phi += (y[t] - x_est[t]) * (x_est[t-1] - mu) / np.exp(x_est[t])\n",
        "            grad_sigma_eta += (y[t] - mu - phi * y[t-1])**2 / (2 * sigma_eta**3)\n",
        "\n",
        "        grad_mu -= (mu - self.mu_prior[0]) / self.mu_prior[1] ** 2\n",
        "        grad_phi -= (phi - self.phi_prior[0]) / self.phi_prior[1] ** 2\n",
        "        grad_sigma_eta -= (sigma_eta - self.sigma_eta_prior[0]) / self.sigma_eta_prior[1] ** 2\n",
        "\n",
        "        return np.array([grad_mu.item(), grad_phi.item(), grad_sigma_eta.item()])\n",
        "\n",
        "    def leapfrog(self, params, momentum, grad, epsilon):\n",
        "        momentum_half_step = momentum - 0.5 * epsilon * grad\n",
        "        params_full_step = params + epsilon * momentum_half_step\n",
        "        grad_new = self.gradient_potential_energy(params_full_step, log_returns)\n",
        "        momentum_full_step = momentum_half_step - 0.5 * epsilon * grad_new\n",
        "        return params_full_step, momentum_full_step, grad_new\n",
        "\n",
        "    def hmc_step(self, params, y):\n",
        "        momentum = np.random.normal(size=params.shape)\n",
        "        current_params = params.copy()\n",
        "        current_momentum = momentum.copy()\n",
        "        current_grad = self.gradient_potential_energy(params, y)\n",
        "        proposed_params = params.copy()\n",
        "        proposed_momentum = momentum.copy()\n",
        "\n",
        "        for _ in range(self.L):\n",
        "            proposed_params, proposed_momentum, current_grad = self.leapfrog(proposed_params, proposed_momentum, current_grad, self.epsilon)\n",
        "\n",
        "        current_potential = self.potential_energy(current_params, y)\n",
        "        current_kinetic = 0.5 * np.sum(current_momentum ** 2)\n",
        "        proposed_potential = self.potential_energy(proposed_params, y)\n",
        "        proposed_kinetic = 0.5 * np.sum(proposed_momentum ** 2)\n",
        "\n",
        "        if np.random.uniform() < np.exp(current_potential - proposed_potential + current_kinetic - proposed_kinetic):\n",
        "            return proposed_params\n",
        "        else:\n",
        "            return current_params\n",
        "\n",
        "    def fit(self, y):\n",
        "        params = np.array([self.mu_prior[0], self.phi_prior[0], self.sigma_eta_prior[0]])\n",
        "        samples = np.zeros((self.n_iter, 3))\n",
        "        for i in range(self.n_iter):\n",
        "            params = self.hmc_step(params, y)\n",
        "            samples[i] = params\n",
        "        return samples\n",
        "\n",
        "# Example usage\n",
        "log_returns = np.random.normal(0, 0.1, 100)\n",
        "F = np.array([[0.9]])  # Initial transition matrix\n",
        "H = np.array([[1]])    # Observation matrix\n",
        "Q = np.array([[0.01]]) # Process noise covariance\n",
        "R = np.array([[1]])    # Observation noise covariance\n",
        "\n",
        "hmc_ekf_model = BayesianStochasticVolatilityEKF_HMC(F, H, Q, R, mu_prior=(0, 1), phi_prior=(0.9, 0.1), sigma_eta_prior=(0.1, 0.01), n_iter=1000, epsilon=0.01, L=10)\n",
        "samples = hmc_ekf_model.fit(log_returns)\n",
        "\n",
        "print(\"Posterior mean estimates\")\n",
        "print(\"mu:\", np.mean(samples[:, 0]))\n",
        "print(\"phi:\", np.mean(samples[:, 1]))\n",
        "print(\"sigma_eta:\", np.mean(samples[:, 2]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTX2XNovXD-Y",
        "outputId": "a8ebe2df-f273-4b94-d263-c435a2a54362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Posterior mean estimates\n",
            "mu: 0.0\n",
            "phi: 0.9000000000000002\n",
            "sigma_eta: 0.10000000000000002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class EKF:\n",
        "    def __init__(self, F, H, Q, R):\n",
        "        self.F = F\n",
        "        self.H = H\n",
        "        self.Q = Q\n",
        "        self.R = R\n",
        "\n",
        "    def predict(self, x, P):\n",
        "        x_pred = self.F @ x\n",
        "        P_pred = self.F @ P @ self.F.T + self.Q\n",
        "        return x_pred, P_pred\n",
        "\n",
        "    def update(self, x_pred, P_pred, y):\n",
        "        y_pred = self.H @ x_pred\n",
        "        S = self.H @ P_pred @ self.H.T + self.R\n",
        "        K = P_pred @ self.H.T @ np.linalg.inv(S)\n",
        "        x_upd = x_pred + K @ (y - y_pred)\n",
        "        P_upd = P_pred - K @ self.H @ P_pred\n",
        "        return x_upd, P_upd\n",
        "\n",
        "    def run(self, y, x0, P0):\n",
        "        n = len(y)\n",
        "        x_est = np.zeros((n, len(x0)))\n",
        "        x_est[0] = x0\n",
        "        P = P0\n",
        "        for t in range(1, n):\n",
        "            x_pred, P_pred = self.predict(x_est[t-1], P)\n",
        "            x_est[t], P = self.update(x_pred, P_pred, y[t])\n",
        "        return x_est\n",
        "\n",
        "class BayesianStochasticVolatilityEKF_HMC:\n",
        "    def __init__(self, F, H, Q, R, mu_prior, phi_prior, sigma_eta_prior, n_iter=1000, epsilon=0.01, L=10):\n",
        "        self.ekf = EKF(F, H, Q, R)\n",
        "        self.mu_prior = mu_prior\n",
        "        self.phi_prior = phi_prior\n",
        "        self.sigma_eta_prior = sigma_eta_prior\n",
        "        self.n_iter = n_iter\n",
        "        self.epsilon = epsilon  # Step size\n",
        "        self.L = L  # Number of leapfrog steps\n",
        "\n",
        "    def potential_energy(self, params, y):\n",
        "        mu, phi, sigma_eta = params\n",
        "        if sigma_eta <= 0:\n",
        "            return np.inf\n",
        "        F = np.array([[phi]])\n",
        "        H = np.array([[1]])\n",
        "        Q = np.array([[sigma_eta**2]])\n",
        "        R = np.array([[1]])  # Assume observation noise is 1 for simplicity\n",
        "\n",
        "        x0 = np.array([mu])\n",
        "        P0 = np.array([[sigma_eta**2]])\n",
        "\n",
        "        x_est = self.ekf.run(y, x0, P0)\n",
        "\n",
        "        likelihood = -0.5 * np.sum((y - x_est.flatten()) ** 2)\n",
        "        prior = -0.5 * (mu - self.mu_prior[0]) ** 2 / self.mu_prior[1] ** 2 \\\n",
        "                -0.5 * (phi - self.phi_prior[0]) ** 2 / self.phi_prior[1] ** 2 \\\n",
        "                -0.5 * (sigma_eta - self.sigma_eta_prior[0]) ** 2 / self.sigma_eta_prior[1] ** 2\n",
        "        return -(likelihood + prior)\n",
        "\n",
        "    def gradient_potential_energy(self, params, y):\n",
        "        mu, phi, sigma_eta = params\n",
        "        if sigma_eta <= 0:\n",
        "            return np.array([0, 0, 0])\n",
        "        F = np.array([[phi]])\n",
        "        H = np.array([[1]])\n",
        "        Q = np.array([[sigma_eta**2]])\n",
        "        R = np.array([[1]])\n",
        "\n",
        "        x0 = np.array([mu])\n",
        "        P0 = np.array([[sigma_eta**2]])\n",
        "\n",
        "        x_est = self.ekf.run(y, x0, P0)\n",
        "\n",
        "        grad_mu = grad_phi = grad_sigma_eta = 0\n",
        "        for t in range(1, len(y)):\n",
        "            grad_mu += (y[t] - x_est[t]) / np.exp(x_est[t])\n",
        "            grad_phi += (y[t] - x_est[t]) * (x_est[t-1] - mu) / np.exp(x_est[t])\n",
        "            grad_sigma_eta += (y[t] - x_est[t]) * np.random.normal(0, sigma_eta) / np.exp(x_est[t])\n",
        "\n",
        "        grad_mu -= (mu - self.mu_prior[0]) / self.mu_prior[1] ** 2\n",
        "        grad_phi -= (phi - self.phi_prior[0]) / self.phi_prior[1] ** 2\n",
        "        grad_sigma_eta -= (sigma_eta - self.sigma_eta_prior[0]) / self.sigma_eta_prior[1] ** 2\n",
        "\n",
        "        return np.array([-grad_mu, -grad_phi, -grad_sigma_eta])\n",
        "\n",
        "    def leapfrog(self, params, momentum, grad, epsilon):\n",
        "        momentum_half_step = momentum - 0.5 * epsilon * grad\n",
        "        params_full_step = params + epsilon * momentum_half_step\n",
        "        grad_new = self.gradient_potential_energy(params_full_step, log_returns)\n",
        "        momentum_full_step = momentum_half_step - 0.5 * epsilon * grad_new\n",
        "\n",
        "        # Ensure parameters stay within valid ranges\n",
        "        params_full_step[2] = max(params_full_step[2], 1e-6)  # sigma_eta must be positive\n",
        "\n",
        "        return params_full_step, momentum_full_step, grad_new\n",
        "\n",
        "    def hmc_step(self, params, y):\n",
        "        momentum = np.random.normal(size=params.shape)\n",
        "        current_params = params.copy()\n",
        "        current_momentum = momentum.copy()\n",
        "        current_grad = self.gradient_potential_energy(params, y)\n",
        "        proposed_params = params.copy()\n",
        "        proposed_momentum = momentum.copy()\n",
        "\n",
        "        for _ in range(self.L):\n",
        "            proposed_params, proposed_momentum, current_grad = self.leapfrog(proposed_params, proposed_momentum, current_grad, self.epsilon)\n",
        "\n",
        "        current_potential = self.potential_energy(current_params, y)\n",
        "        current_kinetic = 0.5 * np.sum(current_momentum ** 2)\n",
        "        proposed_potential = self.potential_energy(proposed_params, y)\n",
        "        proposed_kinetic = 0.5 * np.sum(proposed_momentum ** 2)\n",
        "\n",
        "        if np.random.uniform() < np.exp(current_potential - proposed_potential + current_kinetic - proposed_kinetic):\n",
        "            return proposed_params\n",
        "        else:\n",
        "            return current_params\n",
        "\n",
        "    def fit(self, y):\n",
        "        params = np.array([self.mu_prior[0], self.phi_prior[0], self.sigma_eta_prior[0]])\n",
        "        samples = np.zeros((self.n_iter, 3))\n",
        "        for i in range(self.n_iter):\n",
        "            params = self.hmc_step(params, y)\n",
        "            samples[i] = params\n",
        "        return samples\n",
        "\n",
        "# Example usage\n",
        "log_returns = np.random.normal(0, 0.1, 100)\n",
        "F = np.array([[0.9]])  # Initial transition matrix\n",
        "H = np.array([[1]])    # Observation matrix\n",
        "Q = np.array([[0.01]]) # Process noise covariance\n",
        "R = np.array([[1]])    # Observation noise covariance\n",
        "\n",
        "hmc_ekf_model = BayesianStochasticVolatilityEKF_HMC(F, H, Q, R, mu_prior=(0, 1), phi_prior=(0.9, 0.1), sigma_eta_prior=(0.1, 0.01), n_iter=1000, epsilon=0.01, L=10)\n",
        "samples = hmc_ekf_model.fit(log_returns)\n",
        "\n",
        "print(\"Posterior mean estimates\")\n",
        "print(\"mu:\", np.mean(samples[:, 0]))\n",
        "print(\"phi:\", np.mean(samples[:, 1]))\n",
        "print(\"sigma_eta:\", np.mean(samples[:, 2]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "pBq8Pue2XPyk",
        "outputId": "ef954c15-4745-43ba-abc5-457a67458ac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-b4e573abe291>\u001b[0m in \u001b[0;36m<cell line: 137>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0mhmc_ekf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBayesianStochasticVolatilityEKF_HMC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_eta_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhmc_ekf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_returns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Posterior mean estimates\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-b4e573abe291>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhmc_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-b4e573abe291>\u001b[0m in \u001b[0;36mhmc_step\u001b[0;34m(self, params, y)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0mproposed_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposed_momentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleapfrog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproposed_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposed_momentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mcurrent_potential\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpotential_energy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-b4e573abe291>\u001b[0m in \u001b[0;36mleapfrog\u001b[0;34m(self, params, momentum, grad, epsilon)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mmomentum_half_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mparams_full_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmomentum_half_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mgrad_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_potential_energy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_full_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_returns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mmomentum_full_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmomentum_half_step\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-b4e573abe291>\u001b[0m in \u001b[0;36mgradient_potential_energy\u001b[0;34m(self, params, y)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgradient_potential_energy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_eta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0msigma_eta\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: Hybrid Model Integration**\n",
        "Finally, we integrate the SVM with the adaptive kernel and the Bayesian stochastic volatility model."
      ],
      "metadata": {
        "id": "dNhiJLwWU8KL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hybrid model combining SVM and Bayesian Stochastic Volatility\n",
        "class HybridModel:\n",
        "    def __init__(self, C=1.0, mu=0.0, phi=0.9, sigma_eta=0.1):\n",
        "        self.svm = SVM(C)\n",
        "        self.bsv = BayesianStochasticVolatilityEKF_HMC(mu, phi, sigma_eta)\n",
        "\n",
        "    def fit(self, X, y, log_returns):\n",
        "        h = self.bsv.fit(log_returns)\n",
        "        sigma_func = lambda i, j: np.sqrt(np.exp(h[i] + h[j]))\n",
        "        self.svm.fit(X, y, sigma_func)\n",
        "\n",
        "    def predict(self, X):\n",
        "        h = self.bsv.fit(np.zeros(len(X)))  # Dummy h values for prediction\n",
        "        sigma_func = lambda i, j: np.sqrt(np.exp(h[i] + h[j]))\n",
        "        return self.svm.predict(X, sigma_func)\n",
        "\n",
        "# Example usage\n",
        "log_returns = np.random.normal(0, 0.1, 100)\n",
        "hybrid_model = HybridModel()\n",
        "hybrid_model.fit(X, y, log_returns)\n",
        "hybrid_predictions = hybrid_model.predict(X)\n",
        "print(hybrid_predictions)\n"
      ],
      "metadata": {
        "id": "Aw24_mA1K-vp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "e155eb29-aa06-4129-9274-58f5f93cdcde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BayesianStochasticVolatilityEKF_HMC.__init__() missing 4 required positional arguments: 'R', 'mu_prior', 'phi_prior', and 'sigma_eta_prior'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-a4820e2441a2>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mlog_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mhybrid_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHybridModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mhybrid_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_returns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mhybrid_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhybrid_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-a4820e2441a2>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, C, mu, phi, sigma_eta)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_eta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbsv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBayesianStochasticVolatilityEKF_HMC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_eta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_returns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BayesianStochasticVolatilityEKF_HMC.__init__() missing 4 required positional arguments: 'R', 'mu_prior', 'phi_prior', and 'sigma_eta_prior'"
          ]
        }
      ]
    }
  ]
}